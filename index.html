<!DOCTYPE html>
<!-- saved from url=(0030)https://geng-haoran.github.io/ -->
<html lang="en"><!-- head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    (function () {
      var a_idx = 0;
      window.onclick = function (event) {
        var a = new Array("‚ú®", "ü§ñ", "ü•≥", "üëã", "üíì", "üêΩ", "üëΩ", "üòª", "üí™");

        var heart = document.createElement("b");
        heart.onselectstart = new Function('event.returnValue=false');

        document.body.appendChild(heart).innerHTML = a[a_idx];
        a_idx = (a_idx + 1) % a.length;
        heart.style.cssText = "position: fixed;left:-100%;";

        var f = 16,
          x = event.clientX - f / 2,
          y = event.clientY - f,
          c = randomColor(),
          a = 1,
          s = 1.2;

        var timer = setInterval(function () {
          if (a <= 0) {
            document.body.removeChild(heart);
            clearInterval(timer);
          } else {
            heart.style.cssText = "font-size:16px;cursor: default;position: fixed;color:" +
              c + ";left:" + x + "px;top:" + y + "px;opacity:" + a + ";transform:scale(" +
              s + ");";

            y--;
            a -= 0.016;
            s += 0.002;
          }
        }, 15)

      }
      function randomColor() {

        return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math
          .random() * 255)) + ")";

      }
    }());
  </script>

  <!-- Google tag (gtag.js) -->
  <script async src="./style/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-66DNLPJ6PY');
  </script>

  <title>Mingshuang Luo | ÁΩóÊòéÂèå</title>

  <!-- <meta name="author" content="Mingshuang Luo">ÁΩóÊòéÂèåmeta name="viewport" content="width=device-width, initial-scale=1"> -->

  <link rel="stylesheet" type="text/css" href="./style/stylesheet.css">
  <link rel="icon" type="image/png" href="./style/icon.png">
  <script type="text/javascript">
    function hideshow(which) {
      if (!document.getElementById)
        return
      if (which.style.display == "block")
        which.style.display = "none"
      else
        which.style.display = "block"
    }
  </script>
  <meta http-equiv="origin-trial"
    content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9">
</head>

<!-- bib hide -->

<!-- body -->

<body data-new-gr-c-s-check-loaded="14.1056.0" data-gr-ext-installed>
  <!-- self-intro -->
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2%;width:55%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Mingshuang Luo | ÁΩóÊòéÂèå</name>
                  </p>
                  <p style="text-align: justify">
                    <intro>
                      I am a last year Ph.D. student at <b> the Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS)</b>, China, under the supervision of <a
                        href="https://scholar.google.com/citations?user=LX6MnNsAAAAJ&hl=en">Prof. Hong Chang</a> and <a
                        href="https://scholar.google.com/citations?user=VfS4cisAAAAJ&hl=en">Associate Prof. Ruibing Hou</a>. 

                      
                      <p>Before embarking on my doctoral studies, I worked as a Speech Recognition Algorithm Engineer at <b>Xiaomi Group AI Lab</b> from 2021.7 to 2022.8, supervised by <a href="https://scholar.google.com/citations?user=y_-5FWAAAAAJ&hl=zh-CN&oi=ao">Dr. Daniel Povey (IEEE Fellow)</a>.
                        I obtained my master degree in computer technology from <b>University of the Chinese Academy of Science</b> in 2021.7, supervised by <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=en">Prof. Shiguang Shan (IEEE Fellow)</a> and <a href="https://scholar.google.com/citations?user=8wizL74AAAAJ&hl=en">Associate Prof. Shuang Yang.</p>

                      My research interests lie in computer vision and computer graphics, with a particular focus on
                      Multimodal AIGC, 2D/3D character animation, video/motion generation, world model, and VLA.
                    </intro>
                  <p>
                    Expected graduation in 2026, open to postdoc and
                    research scientist opportunities.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:luomingshuang841@gmail.com">Email</a>
                    &nbsp;/&nbsp;
                    <!-- <a href="data/ZhangJiaxu-CV-2026.pdf">CV</a>
                    &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=LeSGNnMAAAAJ&hl=zh-CN">Google
                      Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/luomingshuang">Github</a>
                    <!-- &nbsp;/&nbsp;
                    <a href="images/wechat.jpg">WeChat</a> -->
                  </p>
                </td>
                <td style="padding:0%; width:26%; max-width:26%;">
                  <br>
                  <!-- Carousel for profile photos -->
                  <div style="width:100%; max-width:180px; margin:auto; position:relative;">
                    <div id="profileCarousel" style="overflow:hidden; position:relative;">
                      <img src="./images/lms.jpg" alt="profile gif" class="hoverZoomLink"
                        style="width:100%; display:block; cursor:pointer;" id="carouselImg" onclick="nextCarousel()">
                    </div>
                    <!-- Carousel emoji buttons below the image -->
                    <!-- <div style="display:flex; justify-content:center; align-items:center; margin-top:10px; gap:16px;">
                      <span id="carouselDot0" style="font-size:20px; cursor:pointer;" onclick="setCarousel(0)">ü•∞</span>
                      <span id="carouselDot1" style="font-size:20px; cursor:pointer;" onclick="setCarousel(1)">ü§°</span>
                    </div> -->
                  </div>
                  <script>
                    const carouselImages = [
                      { src: "./images/lms.jpg", alt: "profile jpg" },
                      // { src: "./images/jiaxuzhang.gif", alt: "profile gif" }
                    ];
                    let carouselIdx = 0;
                    function updateCarousel() {
                      const img = document.getElementById('carouselImg');
                      img.src = carouselImages[carouselIdx].src;
                      img.alt = carouselImages[carouselIdx].alt;
                      // Update emoji style (bold for selected)
                      for (let i = 0; i < carouselImages.length; i++) {
                        const dot = document.getElementById('carouselDot' + i);
                        if (dot) {
                          dot.style.filter = (i === carouselIdx) ? "drop-shadow(0 0 2px #fba524)" : "none";
                          dot.style.opacity = (i === carouselIdx) ? "1" : "0.5";
                        }
                      }
                    }
                    function setCarousel(idx) {
                      carouselIdx = idx;
                      updateCarousel();
                    }
                    function nextCarousel() {
                      carouselIdx = (carouselIdx + 1) % carouselImages.length;
                      updateCarousel();
                    }
                    // Initialize emoji on page load
                    updateCarousel();
                  </script>
                </td>
                <!-- <td style="padding:5%;width:37%;max-width:37%"> 
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/face3.jpg" class="hoverZoomLink">
              <a href="https://hits.seeyoufarm.com" target="_blank"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgeng-haoran.github.io&count_bg=%23FF8400&title_bg=%23545353&icon=tableau.svg&icon_color=%23F3F209&title=hits&edge_flat=false"/></a>
            </td> -->
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            </tbody>
          </table>

          <!-- News -->
          <heading>News</heading><br>
          <div class="news-container">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <!-- <br> -->

                <tr>
                  <td style="padding:0px;width:100%;vertical-align:middle">
                    <p></p>
                    <li>[2026/02] üéâ <a href="https://arxiv.org/abs/2601.21716">DreamActor-M2</a> and its <a href="https://grisoon.github.io/DreamActor-M2/">project page</a> was released.</li>
                    <p></p>
                    <p></p>
                    <li>[2026/01] üéâ <a href="https://arxiv.org/abs/2601.13133">CLASP</a> was accepted by TMM 2026.</li>
                    <p></p>
                    <p></p>
                    <li>[2025/06] üéâ <a href="https://arxiv.org/abs/2411.14951">Morph</a> was accepted by ICCV 2025.</li>
                    <p></p>
                    <p></p>
                    <li>[2024/09] üéâ <a href="https://arxiv.org/abs/2411.14951">M<sup>3</sup>GPT</a> was accepted by NeurIPS 2024.</li>
                    <p></p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <script>
            const newsContainer = document.querySelector('.news-container');
            let isScrolling;

            newsContainer.addEventListener('scroll', () => {
              newsContainer.classList.add('scrolling');
              window.clearTimeout(isScrolling);
              isScrolling = setTimeout(() => {
                newsContainer.classList.remove('scrolling');
              }, 1000);
            });
          </script>

          <!-- Research -->
          <br>
          <heading>Research</heading><br>
          <p>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!--  ----------------------------------------------------------------------------------------------------------------------------------- -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/dreamactor-m2.png" width="200" height="150">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://grisoon.github.io/DreamActor-M2/">
                    <span class="papertitle">DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</span>
                  </a>
                  <br>
                  <strong>Mingshuang Luo*</strong>, Shuang Liang*, Zhengkun Rong*, Yuxuan Luo‚Ä†, Tianshu Hu¬ß, Ruibing Hou¬ß, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao
                  <br>
                  <em>Tech Report</em>, 2026
                  <br>
                  <a href="DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning">project
                    page </a> / <a href="https://arxiv.org/abs/2601.21716">arxiv </a>
                  <p></p>
                  <p>We present DreamActor-M2, a universal character image animation framework that reformulates motion conditioning as a spatiotemporal in-context learning task. Our design harnesses the inherent generative priors of video foundation models while facilitating a critical evolution toward pose-free, end-to-end motion transfer directly from raw videos.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/flowact-r1.png" width="200" height="150">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://grisoon.github.io/FlowAct-R1/">
                    <span class="papertitle">FlowAct-R1: Towards Interactive Humanoid Video Generation</span>
                  </a>
                  <br>
                  <strong>FlowAct Team, ByteDance Intelligent Creation</strong>
                  <br>
                  <em>Tech Report</em>, 2026
                  <br>
                  <a href="https://grisoon.github.io/FlowAct-R1/">project
                    page </a> / <a href="https://arxiv.org/abs/2601.10103">arxiv </a>
                  <p></p>
                  <p>We present FlowAct-R1, a novel framework that enables lifelike, responsive, and high-fidelity
                    humanoid video generation for seamless real-time interaction.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/clasp.png" width="200" height="150">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2601.13133">
                    <span class="papertitle">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</span>
                  </a>
                  <br>
                  <strong>Mingshuang Luo</sÁΩóÊòéÂèång></strong>, Ruibing Hou*, Bo Chao, Hong Chang, Zimo Liu*, Yaowei Wang, Shiguang Shan
                  <br>
                  <em>TMM</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2601.13133">project
                    page </a> / <a href="https://arxiv.org/abs/2601.13133">arxiv </a>
                  <p></p>
                  <p>We propose CLASP (CLIP-guided Adaptable Self-suPervised
                  learning), a novel framework designed for unsupervised pretraining in human-centric visual tasks.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/morph.png" width="200" height="150">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2411.14951">
                    <span class="papertitle">Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</span>
                  </a>
                  <br>
                  Zhuo Li*, <strong></strong>Mingshuang Luo*</sÁΩóÊòéÂèång>, Ruibing Hou‚Ä†, Xin Zhao, Hao Liu, Hong Chang, Zimo Liu, Chen Li
                  <br>
                  
                  <em>ICCV</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2411.14951">project
                    page </a> / <a href="https://arxiv.org/abs/2411.14951">arxiv </a>
                  <p></p>
                  <p>We propose Morph, a Motion-Free physics optimization framework, consisting of a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on expensive real-world motion data. </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/m3gpt.png" width="200" height="150">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2405.16273">
                    <span class="papertitle">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</span>
                  </a>
                  <br>
                  <strong>Mingshuang Luo</sÁΩóÊòéÂèång>, Ruibing Hou*, Zhuo Li, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2405.16273">project
                    page </a> / <a href="https://arxiv.org/abs/2405.16273">arxiv </a>
                  <p></p>
                  <p>We present M<sup>3</sup>GPT, an advanced Multimodal, Multitask framework for Motion comprehension and generation.</p>
                </td>
              </tr>
              <!--  ----------------------------------------------------------------------------------------------------------------------------------- -->

            </tbody>
          </table>

          <!-- Services -->
          <!-- <br><heading>Services</heading><br><table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        
        
        
      <td style="padding:0px;width:100%;vertical-align:middle">
        <p>
          </p><li>Reviewer: SIGGRAPH, NeurIPS, AAAI</li>
        <p></p>
      </td>
    </tr>
  </tbody></table> -->

          <!-- Experience -->
          <br>
          <heading>Experience and Education</heading><br><br>
          <table width="100%" align="center" border="0" cellpadding="10" style="border-spacing: 20px;">
            <tbody>
              <!-- <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/ByteDanceLOGO.png" , width="105"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://www.bytedance.com/" target="_blank">
                      <papertitle>ByteDance</papertitle>
                    </a></strong>
                  <br> <em>2025.06 - Present, Shenzhen</em><br>
                  <strong>Research Intern</strong> for AIGC and MLLM.
                  <br> Advisor: Dr. <a href="https://chenxin.tech/" target="_blank">Xin Chen</a> and Dr. <a
                    href="https://scholar.google.com/citations?user=BIixVT0AAAAJ&hl=en" target="_blank">Tianshu Hu</a>
                </td>
              </tr>

              <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/step_logo.png" , width="105"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://www.stepfun.com/" target="_blank">
                      <papertitle>StepFun</papertitle>
                    </a></strong>
                  <br> <em>2024.05 - 2025.06, Shanghai</em><br>
                  <strong>Research Intern</strong> for AIGC.
                  <br> Advisor: Dr. <a href="https://www.skicyyu.org/" target="_blank">Gang Yu</a> and Dr. <a
                    href="https://scholar.google.com/citations?user=tgDc0fsAAAAJ&hl=en" target="_blank">Xianfang
                    Zeng</a>
                </td>
              </tr>

              <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/tencent_logo.png" , width="105"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://ai.tencent.com/" target="_blank">
                      <papertitle>Tencent</papertitle>
                    </a></strong>
                  <br> <em>2023.06 - 2024.04, Shanghai</em><br>
                  <strong>Research Intern</strong> in Tencent PCG.
                  Advisor: Dr. <a href="https://www.skicyyu.org/" target="_blank">Gang Yu</a> and Dr. <a
                    href="https://chenxin.tech/" target="_blank">Xin
                    Chen</a>
                  <br> <em>2022.07 - 2023.06, Shenzhen</em><br>
                  <strong>Research Intern</strong> in Tencent AI Lab.
                  Advisor: Dr. <a href="https://calcifer.me/" target="_blank">Junwu Weng</a> and Dr. <a
                    href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=zh-CN" target="_blank">Shaoli
                    Huang</a>
                </td>
              </tr> -->

              <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/xiaomi.png" , width="105"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://www.mi.com/index.html" target="_blank">
                      <papertitle>Xiaomi Group</papertitle>
                    </a></strong>
                  <br> <em>2021.07 - 2022.08, Beijing</em><br>
                  <strong>I was a Speech Recognition Algorithm Engineer.</strong>
                  <br> Advisor: Dr. <a href="https://scholar.google.com/citations?user=y_-5FWAAAAAJ&hl=zh-CN&oi=ao"
                    target="_blank">Daniel Povey (IEEE Fellow)</a>
                </td>
              </tr>

              <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/ict.png" , width="105"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://www.ict.ac.cn/" target="_blank">
                      <papertitle>Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS)</papertitle>
                    </a></strong>
                  <br> <em>2022.08 - Present, Beijing</em><br>
                  <strong>I was a PhD Student of Computer Science and Technology.</strong>
                  <br> Research Advisor: Prof. <a href="https://scholar.google.com/citations?user=LX6MnNsAAAAJ&hl=zh-CN"
                    target="_blank">Hong Chan</a>
                </td>
              </tr>

              <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/cas.png" , width="100"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://www.ucas.ac.cn/" target="_blank">
                      <papertitle>University of Chinese
                        Academies of Sciences (UCAS)</papertitle></strong>
                    </a></strong>
                  <br> <em>2018.09 - 2021.07, Beijing</em><br> I received my <strong> Master Degree</strong> of
                    Computer Technology in 2021.
                  <br> Research Advisor: Prof. <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=zh-CN" target="_blank">Shiguang Shan (IEEE Fellow)</a>
                </td>
              </tr>


              <tr>
                <td style="padding-left:18px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="./images/xiangtangdaxue.png" , width="100"></td>
                <td width="80%" valign="center">
                  <strong><a href="https://www.xtu.edu.cn/" target="_blank">
                      <papertitle>XiangTan
                        University</papertitle>
                    </a></strong>
                  <br> <em>2014.09 - 2018.06, Xiangtan</em><br> I received
                  my <strong>B.S Degree</strong> of Electric Information Science and Technology.
                </td>
              </tr>

            </tbody>
          </table>

          <!-- Selected Awards and Honors
          <br>
          <heading>Awards and Honors</heading><br>
          <table
            style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <p>
                  </p>
                  <li>2025: Academic Innovation Award of Wuhan University </a> (15,000RMB¬•,
                    Top 1%)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2024: NSFC Basic Research Project for Youth Scholars
                    </a> (300,000RMB¬•)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2023: <a href="https://www.sohu.com/a/745941721_121123740"> Lei
                      Jun Excellence Scholarship </a> (<strong>100,000RMB¬•,
                      Top 0.1‚Ä∞</strong>)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2023: Wang Zhizhuo Innovative Talent Award
                    (8,000RMB¬•, Top 1%)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2022: National Scholarship (<strong>Highest
                      Honor</strong> for Master students in China,
                    10,000RMB¬•, Top 3%)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2022: First-class Scholarship of Wuhan University
                    (5,000RMB¬•, Top 10%)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2021: First-class Scholarship of Wuhan University
                    (5,000RMB¬•, Top 10%)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2021: 1<sup>st</sup> Runner-up of <a
                      href="https://sutdcv.github.io/multi-modal-video-reasoning/#/leaderboard">ICCV
                      2021 MMVRAC Challenge</a> (Track 2 and Track 3)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2020: Outstanding graduates of Southeast
                    University (Top 3%)</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2019: Meritorious Winner - Mathematical Contest In
                    Modeling & Interdisciplinary Contest In Modeling,
                    2019</li>
                  <p></p>
                  <p>
                  </p>
                  <li>2018: National Scholarship (<strong>Highest
                      Honor</strong> for undergraduates in China, 8,000RMB¬•,
                    Top 3%)</li>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <hr>
          <p align="center">
            <a href="https://clustrmaps.com/site/1bbtb" title="Visit tracker"><img
                src="//clustrmaps.com/map_v2.png?cl=be8670&w=400&t=tt&d=qZS4ggcBQIgR4I9SYbVQoi-6xMTgYyMGlUnPAq-jWv4&co=ffffff&ct=bd5d38" /></a>
          </p>

          <!-- Aknowledgements -->
          <p style="text-align:center">
            This homepage is designed based on <a href="https://jonbarron.info/">Jon Barron</a>'s website and
            deployed on <a href="https://pages.github.com/">Github
              Pages</a>. Last updated: Jan. 2026

            <br>
            ¬© 2026 Mingshuang Luo
ÁΩóÊòéÂèå     </p>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <br> -->
                <!-- <br> -->

              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open">
    <style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select: none;
        user-select: none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style>
    <div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration"
      data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}">
    </div>
  </template></grammarly-desktop-integration>

</html>